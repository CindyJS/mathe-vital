---
title: Approximation mit Polynombasen II
layout: page
category: LinAlgBonbons
---

# Regression bezüglich beliebiger Basisfunktionen

Das eben beschriebene Verfahren lässt sich nicht nur auf lineare Regressionsfunktionen anwenden. Man kann sich einen beliebigen Satz von Basisfunktionen
\\[ f_1(x),f_2(x),\ldots,f_k(x) \\]
vorgeben und nach einer Gewichtung  $a_1,a_2,\ldots,a_k$ suchen, so dass für einen gegebenen Satz von $n$ Daten $(x_i,y_i)$ die Quadratsumme
\\[ \sum_{i=1}^n(f(x_i)-y_i)^2 \\]
minimal wird, wobei die Funktion $f(x)$ eine Linearkombination der Basisfunktionen ist:
\\[ f(x)=a_1f_1(x)+a_1f_1(x)+\cdots+a_kf_k(x) \\]
Im folgenden Beispiel kann man sowohl die Position der Datenpunkte als auch die Basisfunktionen verändern. Das Startbeispiel berechnet eine Regressionsparabel bezüglich der Basis $1$, $x$, $x^2$.


{% capture applet %} {% include_relative images/Regression3.html %} {% endcapture %}
{% include showapplet.html %}

Basis-Funktionen durch Komma getrennt eingeben:  
1,x,x^2
oder Basisfunktionen durch Knopfdruck auswählen:

Die Berechnung erfolgt hierbei vollkommen analog zum Berechnen der Regressionsgeraden zuvor. Mann bestimmt zunächst eine Matrix $M$ und einen Vector $y$ gemäß:
\\[M=\left(\matrix{ f_1(x_1)&amp;f_2(x_1)&amp;\cdots&amp;f_k(x_1)\cr f_1(x_2)&amp;f_2(x_2)&amp;\cdots&amp;f_k(x_2)\cr \vdots&amp;\vdots&amp;\ddots&amp;\vdots\cr f_1(x_n)&amp;f_2(x_n)&amp;\cdots&amp;f_k(x_n)\cr }\right); \qquad y=\left(\matrix{ y_1\cr y_2\cr \vdots\cr y_n\cr }\right) \\]
Die gesuchten Paramter ergeben sich dann durch Lösen des $k\times k$ Gleichungssystems
\[ M^TM\cdot a=M^Ty. \]
Wobei $a=(a_1,a_2,\ldots a_k)$ der gesuchte Parametervektor ist.
